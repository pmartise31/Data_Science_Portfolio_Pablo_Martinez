# -*- coding: utf-8 -*-
"""Data Mining Coursework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X8SV678bM6PzK_-glrfaK79l6gU9-kug

# Coursework Data Mining:

Code instructions:

- There is a commented part in the code which corresponds to a Random Forest Regressor implemented by TensorFlow. There is no way to run it in the Linux terminal. But it still worked in Collab and other environments. So, if it is possible to run it, it will be good.
- In case it is desired to run, write this command:
"""

# !pip install tensorflow_decision_forests

"""## Part 1:

 In this part, you are required to write a Python script that will import a dataset, pre-process it,
 select/extract appropriate features, partition the data space, and create a model for prediction. The
 dataset is described below, and the target for this task is to predict the age of the Abalone.

 The age can
 be calculated by adding 1.5 to the number of rings.
 You will attract marks based on how well your code works, the structure and comments in your code,
 the results you have achieved (including how well the results generalise to my reserved data), and your
 justification of the techniques you have used.

**STEP** **1**: Load the data and inspect it:
"""

import pandas as pd

file_name = "abalone.head" # adding the relative path
df = pd.read_csv(file_name, sep = ",", header = None) # reading the file

# As the column names have not been added we add them manually:
df.columns = ["Sex", "Length", "Diameter", "Height", "Whole weight", "Stucked weight", "Viscera weight", "Shell weight", "Number of rings"]

# We are also going to add into that step the Age variable, which simply consists of adding 1.5 to the number of rings:
df["Age"] = df["Number of rings"] + 1.5

df.head()# inspecting the first 5 observations

"""The relatioship between the age and the number of rings is completely deterministic, so that the age can be computed through the number of rings perfectly without having to approximate complex relationships among data. This analysis will be based on predicting the age towards the other existing parameters of the abalone dataset.

**STEP 2**: Check the types of data:
"""

# Inspecting the types of data of our variables set:
df.dtypes

"""All the datatypes seem to be consistent to carry on with the analysis.

**STEP 3**: Anomaly detection. Anomalies are also defined as outliers, they are the observations in the data that are inconsistent compared to the remaining data points.
"""

import matplotlib.pyplot as plt

# A good method to detect an outlier is by data visualization through outliers:
fig, axs = plt.subplots(2,4)

# We will perform the analysis only in the numerical variables as Sex variable makes no sense to include it except for the number of rings,
# which will show a similar distribution than the age variable:

axs[0,0].boxplot(df["Length"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[0,0].set_title("Length")
axs[0,1].boxplot(df["Diameter"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[0,1].set_title("Diameter")
axs[0,2].boxplot(df["Height"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[0,2].set_title("Height")
axs[0,3].boxplot(df["Whole weight"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[0,3].set_title("Whole weight")
axs[1,0].boxplot(df["Stucked weight"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[1,0].set_title("Stucked weight")
axs[1,1].boxplot(df["Viscera weight"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[1,1].set_title("Viscera weight")
axs[1,2].boxplot(df["Shell weight"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[1,2].set_title("Shell weight")
axs[1,3].boxplot(df["Age"], patch_artist = True, boxprops = dict(facecolor  = "lightblue"))
axs[1,3].set_title("Age")
for ax in axs.flatten():
  ax.axis("off")
plt.tight_layout()
plt.show()

"""As we can see, our dataset shows variables containing a considerable number of outliers. These outliers lead to poor model performance as well as overfitting on these extreme values and avoid generalization of the model.

However, data visualization tools are not the only existing possibility for the outlier detection and removal. I will explore also outlier detection through statistical techniques, in this case the Z-score normalization and IQR.

But first let's plot the histograms and qqplots to observe the data distribution of all the predictors:

https://seaborn.pydata.org/generated/seaborn.histplot.html

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html
"""

# Before outlier detection let's check wether our data is normally distributed or not:
import seaborn as sns
from scipy.stats import probplot

# These are the histograms and the qqplot for the variables Lenght and Diameter
plt.figure(figsize = (10,6))

plt.subplot(2,2,1)
sns.histplot(df["Length"],kde = True, bins = 15, color = "red") # plotting the histograms
plt.title("Histogram for Lenght")
plt.subplot(2,2,2)
probplot(df["Length"], dist = "norm", plot = plt) # plotting the qqplots
plt.title("Probplot for Lenght")
plt.subplot(2,2,3)
sns.histplot(df["Diameter"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Diameter")
plt.subplot(2,2,4)
probplot(df["Diameter"], dist = "norm", plot = plt)
plt.title("Probplot for Diameter")

plt.tight_layout()
plt.show()

# These are the histograms and the qqplot for the variables Height and Whole weight
plt.figure(figsize = (10,6))

plt.subplot(2,2,1)
sns.histplot(df["Height"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Height")
plt.subplot(2,2,2)
probplot(df["Height"], dist = "norm", plot = plt)
plt.title("Probplot for Height")
plt.subplot(2,2,3)
sns.histplot(df["Whole weight"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Whole weight")
plt.subplot(2,2,4)
probplot(df["Whole weight"], dist = "norm", plot = plt)
plt.title("Probplot for Whole weight")

plt.tight_layout()
plt.show()

# These are the histograms and the qqplot for the variables Stucked weight and Viscera weight
plt.figure(figsize = (10,6))

plt.subplot(2,2,1)
sns.histplot(df["Stucked weight"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Stucked weight")
plt.subplot(2,2,2)
probplot(df["Stucked weight"], dist = "norm", plot = plt)
plt.title("Probplot for Stucked weight")
plt.subplot(2,2,3)
sns.histplot(df["Viscera weight"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Viscera weight")
plt.subplot(2,2,4)
probplot(df["Viscera weight"], dist = "norm", plot = plt)
plt.title("Probplot for Viscera weight")

plt.tight_layout()
plt.show()

# These are the histograms and the qqplot for the variables Shell weight and Age
plt.figure(figsize = (10,6))

plt.subplot(2,2,1)
sns.histplot(df["Shell weight"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Shell weight")
plt.subplot(2,2,2)
probplot(df["Shell weight"], dist = "norm", plot = plt)
plt.title("Probplot for Shell weight")
plt.subplot(2,2,3)
sns.histplot(df["Age"],kde = True, bins = 15, color = "red")
plt.title("Histogram for Age")
plt.subplot(2,2,4)
probplot(df["Age"], dist = "norm", plot = plt)
plt.title("Probplot for Age")

plt.tight_layout()
plt.show()

"""*Interpretation of the graphs*:

Even though all the data distributions are different there is a general trend that can be extracted from data:

*   The histogram observations suggest that the data doesn't exactly follow a normal distribution, but the histograms are bell-shaped.
*   Concerning the qqplot, the points lie in a certain extent in the red line. This behaviour is observed especially in the middle of the distribution. In the extremes, the deviation from the red line of the data points states the outliers presence.  

Once having described this general behavior, another dataset without outliers will be prepared to perform the model with it. The idea is to detect and delete the outliers through the IQR technique. Then the 2 datasets, the original and the one without outliers will be compared to see which one is preferable to be applied to unseen data.

Another technique, the Z-score normalization, was also applied, providing different extreme values to each of the variables depending on their data distribution. However, as this techinque slightly relies on subjectivity when it comes to decide the boundary values, the IQR will be the one used for the final analysis.

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html


"""

# Z-score for each of the variables:
from scipy.stats import zscore
import numpy as np

# We create a function to compute the z-score and provide upper and lower thresholds:
def z_score(var, lt, rt):
  ''' In this function the lt stands for the left tail threshold and the rt for the right tail threshold '''
  z = zscore(var) # computing the z_score
  index = np.where((z < lt) | (z > rt)) # extracting the indexes where the outliers are set
  return index
# Z-score of the variables:

lenght_index = z_score(df["Length"], -2.5, 3)[0] # The lenght shows a negative skewness (the left tail is longer)
diameter_index = z_score(df["Diameter"], -2.5, 3)[0] # the diameter also shows a longer left tail
height_index = z_score(df["Height"], -3, 4)[0] # the height shows a very pronounced right tail
whole_weight_index = z_score(df["Whole weight"], -3, 3.5)[0] # the whole weight shows also a right tail, in fact it will be the same behavior for all the wieght variables
stucked_weight_index = z_score(df["Stucked weight"], -3, 3.5)[0]
viscera_weight_index = z_score(df["Viscera weight"], -3, 3.5)[0]
shell_weight_index = z_score(df["Shell weight"], -3, 3.5)[0]
age_index = z_score(df["Age"], -3, 4)[0] # Age and number of rings also presents a positive skewness
rings_index = z_score(df["Number of rings"], -3, 4)[0]

"""Outlier removal by the interquantile range (IQR):

https://datagy.io/pandas-iqr/

https://www.geeksforgeeks.org/interquartile-range-to-detect-outliers-in-data/
"""

def IQR(var):
  '''This function computes the IQR of a data distribution and return the lower and upper limits for outlier detection'''
  Q1 = np.percentile(var, 25, interpolation = "midpoint") # computing the first quartile (25%)
  Q3 = np.percentile(var, 75, interpolation = "midpoint") # computing the third quartile (75%)
  IQR = Q3-Q1 #Interquantile range calculation
  # Computing the low and up limits for outlier consideration:
  low_lim = Q1 - 1.5*IQR
  up_lim = Q3 + 1.5*IQR
  outliers_index = [] # creating an empty list for storing the outliers indexes
  for i in range(var.shape[0]): # running through the variable values in the df
    if (var[i] < low_lim) | (var[i] > up_lim): #assigning as an outlier the value which is out defined limits
      outliers_index.append(i) # storing the indexes fullfilling the conditions
  return outliers_index

# Storing the indexes where the outliers are present for all the variables
lenght_index = IQR(df["Length"])
diameter_index = IQR(df["Diameter"])
height_index = IQR(df["Height"])
whole_weight_index = IQR(df["Whole weight"])
stucked_weight_index = IQR(df["Stucked weight"])
viscera_weight_index = IQR(df["Viscera weight"])
shell_weight_index = IQR(df["Shell weight"])
age_index = IQR(df["Age"])
rings_index = IQR(df["Number of rings"])

"""Now the strategy followed will consist on:


*   Copying the arrays for all the variables of the original dataframe.
*   Deleting the outliers for each variable.
*   Computing the mean for each variable without the outliers.
*   Replacing the outliers in the original dataset by the mean values.

I consider this procedure valid as I assume that the data is symmetric and so replacing the outliers by the mean is a common and correct approach as a data imputation technique.
"""

# Let's create a function that performs the desired procedure:
def mean_imputation(dfo, column, indexes):
  ''' Computing the mean of the variables without accounting for the outliers'''
  clean_vec = dfo[column].drop(indexes) # storing in a vector the variable observations dropping the outliers
  mean_val = np.mean(clean_vec) # computing the mean
  return mean_val

# Performing the clean mean calculation for all the variables:
lenght_mean = mean_imputation(df, "Length", lenght_index)
diameter_mean = mean_imputation(df, "Diameter", diameter_index)
height_mean = mean_imputation(df, "Height", height_index)
whole_weight_mean = mean_imputation(df, "Whole weight", whole_weight_index)
stucked_weight_mean = mean_imputation(df, "Stucked weight", stucked_weight_index)
viscera_weight_mean = mean_imputation(df, "Viscera weight", viscera_weight_index)
shell_weight_mean = mean_imputation(df, "Shell weight", shell_weight_index)
age_mean = mean_imputation(df, "Age", age_index)
rings_mean = mean_imputation(df, "Number of rings", rings_index)

"""Creating the dataframe free of outliers:"""

clean_df = df.copy() # copy of the original dataframe

# Replacing the outliers by the clean mean for all the variables:
clean_df.loc[lenght_index, "Length"] = lenght_mean
clean_df.loc[diameter_index, "Diameter"] = diameter_mean
clean_df.loc[height_index, "Height"] = height_mean
clean_df.loc[whole_weight_index, "Whole weight"] = whole_weight_mean
clean_df.loc[stucked_weight_index, "Stucked weight"] = stucked_weight_mean
clean_df.loc[viscera_weight_index, "Viscera weight"] = viscera_weight_mean
clean_df.loc[shell_weight_index, "Shell weight"] = shell_weight_mean
clean_df.loc[age_index, "Age"] = age_mean
clean_df.loc[rings_index, "Number of rings"] = int(round(rings_mean)) #rounding to the closest integer value to be consistent with the data type

clean_df.head()

"""**STEP 4**: Data Preprocessing (Data cleaning and dealing with missing values)


"""

# Counting the number of Missing Values for each column (variable):
df.isnull().sum()

"""So we don't have to deal with any missing values in our dataset.

**STEP 5**: Data normalization (Feature scalling)

For this step, the min-max normalization is used for both dataframes (with the outliers and without the outliers). This feature scalling technique will transform all the features individually such that they are scaled into a range betweeen {0,1}.

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html


"""

from sklearn.preprocessing import MinMaxScaler

# The feature scalling will be performed by min-max normalization only in the columns which must be normalized:

# For the original dataset:

scaled_columns = df.iloc[:,1:8] # performing only in the numerical variables which are not the number of rings
scaler = MinMaxScaler() # calling the scaler
scaled_df = pd.DataFrame(scaler.fit_transform(scaled_columns), columns = df.columns[1:8]) # transforming the scaled features into a dataframe again


# For the dataset without the outliers:

scaled_columns = clean_df.iloc[:,1:8]
scaler = MinMaxScaler()
scaled_df_out = pd.DataFrame(scaler.fit_transform(scaled_columns), columns = clean_df.columns[1:8])

"""**STEP 6**: Feature engeneering for the categorical variables

Performing conversion for the Sex variable in the dataset. This step consisits of assigning to each class an integer number. This operation is performed because commonly machine learning algorithms work with numerical data.
"""

# Looking together at the 2 different resulting dataframes that we have:

# For the scaled dataset:
scaled_df["Number of rings"] = df["Number of rings"]
scaled_df["Age"] = df["Age"]
scaled_df["Sex"] = df["Sex"]

# For the scaled dataset without outliers:
scaled_df_out["Number of rings"] = clean_df["Number of rings"]
scaled_df_out["Age"] = clean_df["Age"]
scaled_df_out["Sex"] = clean_df["Sex"]

# The idea will be to convert the categorical variable classes to integer values:
# The replace function will replace the categorical values by the numerical:
scaled_df.replace({"M":0.0,"F":1.0,"I":2.0}, inplace = True)
scaled_df_out.replace({"M":0.0,"F":1.0,"I":2.0}, inplace = True)

"""**STEP 7**: Feature correlation inspection

In this step I will check which are the linear relationships between the variables in the dataset (putting especial attention to the Age which is the target variable).

https://seaborn.pydata.org/generated/seaborn.heatmap.html
"""

# We compute the linear correlation among variable in the dataset:
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (10,6))
# A good approach to check the correlation between the variables in a dataframe is to perform a heatmap:
sns.heatmap(scaled_df.corr(), annot = True, fmt = ".1f", cmap = "crest", annot_kws={"size": 15})
plt.title("Heatmap for scaled dataframe")

plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.tight_layout()
plt.show()

"""As it is observed, there is not a linear relationship towards the variables that can be accounted to predict Age, aside from the one provided by the number of rings, which is redundant. There is no variable presenting a value higher enough to decide wether two variables are linearly correlated enough to build a linear predictive model. The conclusion that can be drawn is that there is not linear relationship between the age and the other variables, and so, simple statistical models such as linear regression are discarded for this analysis. Then a regression machine learning model will be a good idea to be applied to this approach.

**STEP 8**: Dataset Splitting

We will split the data into training and test.

https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

# Importing the needed libraries:
from sklearn.model_selection import train_test_split

# Removing the Age and the number of rings as the predictors -> include the number of rings will be redundant in the analysis because we obtained the age through them.
X = scaled_df.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df["Age"] # predicting the age
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 19) # train and test spliting

"""**STEP 9**: Model application: Random Forest Regressor model for predicting the age of the abalones.

Random Forests are the application of multiple decision trees, which are a very common tool used in classification tasks because they are normally easy to apply, fast and insexpensive to construct. However, it is also possible to apply Random Forest algorithms to regression problems. The decision of applying this algorithm relies on the fact that these models perform well with non-linear relationships between the features and the predicted variables. Furtheremore, simple decision trees are models which tend to overfit while random forest is a model which averages the results of all the trees it contains and so generalizes better in unseen data. So a first model is tried by applying the Random Forest Regressor machine learning model to the min-max scalled dataset previously preprocessed.

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

Random Forest Regressor with the default parameters:
"""

# Importing the needed libraries:
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def rfr_algorithm(X_train, X_test, y_train, y_test):

  # Calling the model with the best performing parameters:
  rf_regressor = RandomForestRegressor(random_state = 42)
  rf_regressor.fit(X_train, y_train) # training the model
  y_pred = rf_regressor.predict(X_test) # making predictions on the model

  # Performing the general error functions and performance metrics:
  mse = mean_squared_error(y_test, y_pred) # mean squared error
  r2 = r2_score(y_test, y_pred) # R square metric
  mae = mean_absolute_error(y_pred, y_test) # Mean absolute error

  evaluation_metrics = [mse, r2, mae] # storing the evaluation metrics of default parameters
  # Printing the metrics:
  print(f"Mean Squared Error: {mse:.2f}")
  print(f"R-squared Score: {r2:.2f}")
  print(f"Mean Absolute Error: {mae:.2f}")

  # Printing some comparison between the actual age observations and the predicted
  comparison = pd.DataFrame({"Actual": y_test.reset_index(drop = True), "Predicted": y_pred})
  print(comparison)
  return evaluation_metrics

rfr_default = rfr_algorithm(X_train, X_test, y_train, y_test)

"""The performance of the model at this point is weak, for that reason I will perform 2 different approaches that could help to improve it.

**STEP 10**: Dimensionality Reduction

One of the strategies to try to improve the performance of the model is the dimensionality reduction. Sometimes when provided a high dimensional data model, it is a good approach to try to reduce the number of features, because some of them are adding noise which makes unnecesarily the model more complex than it should be. Both PCA and Feature selection by feature importance will be perfromed to check which has the better influence in the model performance.

First let's check the feature importances:


"""

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Calling the model with the best performing parameters:
rf_regressor = RandomForestRegressor(random_state = 42)
rf_regressor.fit(X_train, y_train) # training the model
y_pred = rf_regressor.predict(X_test) # making predictions on the model

imp = rf_regressor.feature_importances_ #retaining the model feature importances
features = ["Length", "Diameter", "Height", "Whole weight", "Stucked weight", "Viscera weight", "Shell weight", "Sex"] # extracting the feature names
df_importance = pd.DataFrame({"Feature": features, "Importances" : imp})
df_importance

"""So the Shell weight seems to be the feature which carries the more importance in the model predictions.

Then a PCA analysis is performed to check wether the model performance can improve and also if some features can be extracted from the original ones so as to reduce the number of features in the dataset.

https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
"""

#PCA execution:
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(X_train) # fitting the sgtandardized data with the pca
# I compute the cumulated variance to check which is the optimal number of components that retain the maximum variance possible among the data
variance_cumulated = np.cumsum(pca.explained_variance_ratio_)

# The scree plot: Displays the explained variance proportion against the number of components:
plt.plot(range(1,len(pca.explained_variance_ratio_)+1), variance_cumulated)
plt.xlabel("Number of components")
plt.ylabel("Variance cumulated")
plt.title("PCA analysis on the scaled dataframe")
plt.axhline(y = 0.99, color = "black")
plt.axvline( x = 5, color = "black")
plt.show()

"""Now we test the model after applying the PCA to the feature space:"""

X = scaled_df.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df["Age"] # predicting the age

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X= scaler.fit_transform(X)

pca = PCA(n_components = 0.99) # PCA for number of components explaining 99% of the variability in our data
X_pca = pca.fit_transform(X) # fitting the standardized data with the pca
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size = 0.2, random_state = 19) # train and test spliting

rfr_PCA = rfr_algorithm(X_train, X_test, y_train, y_test)

"""As it is seen, the PCA doesn't improve the model performance, but it still slighltly improves the computational expense of the model. However, the fact that PCA is not the best approach to face this dimensionality reduction problem is also true, because PCA tends to be useful when applied towards data presenting linear relationship patterns.  So the alternative approach that will be used is t-SNE. t-SNE is a technique that converts the data points into joint probabilities and minimizes the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high dimensional data. The initializations of the method will determine the obtained results. The following parameters will be changed to perform my analysis:

- n_components: it is the dimension of the embedded space
- perplexity: number of neirest neighbors
- learning rate: controls the size of steps of the algorithm to minimize the loss function
- n_iter: number of the iterations run:

https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html

Only run this part once, it is computationnaly expensive, so maybe is better to don't run it, because it doesn't improve the model but still provides the better hyperparameter combination for it.



"""

from sklearn.manifold import TSNE

X = scaled_df.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df["Age"] # predicting the age

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X= scaler.fit_transform(X)

# For finding the most suitable parameters we will do hyperparameter tuning:

best_model = None #best model parameters list initialization
best_r = 0 #best model r-squared initialization

# Defining a set of parameters to be checked:
parameter_grid = {
    "perplexity":[5,10,30,50],
    "learning_rate":[10,100,500,1000],
    "n_iter":[500,1000,2000]}

# Trying all the different combination of hyperparameters:
for perplexity in parameter_grid["perplexity"]:
  for learning_rate in parameter_grid["learning_rate"]:
    for n_iter in parameter_grid["n_iter"]:
      tsne = TSNE(n_components = 2, learning_rate = learning_rate, perplexity = perplexity, n_iter = n_iter, random_state = 22)
      X_tsne = tsne.fit_transform(X)

      X_train, X_test, y_train, y_test = train_test_split(X_tsne, y, test_size = 0.2, random_state = 19) # train and test spliting

      # Calling the model with the best performing parameters:
      rf_regressor = RandomForestRegressor(n_estimators = 300, max_depth = 10, min_samples_leaf = 4, min_samples_split = 10, random_state = 42)
      rf_regressor.fit(X_train, y_train) # training the model
      y_pred = rf_regressor.predict(X_test) # making predictions on the model

      # Performing the general error functions and performance metrics:
      r2 = r2_score(y_test, y_pred) # R square metric

      if r2>best_r:
        best_r = r2
        mse = mean_squared_error(y_test, y_pred) # mean squared error
        mae = mean_absolute_error(y_pred, y_test) # Mean absolute error
        best_params = [perplexity, learning_rate, n_iter]
      print(best_params)

print(f"R-squared Score: {best_r:.2f}")
rfr_tSNE = [mse, best_r, mae]

"""tSNE applied with the best hyperparameters combination:"""

from sklearn.manifold import TSNE

#The best combination is achieved at : [5, 1000, 500]
X = scaled_df.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df["Age"] # predicting the age

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X= scaler.fit_transform(X)

tsne = TSNE(n_components = 2, learning_rate = 1000, perplexity = 5, n_iter = 500, random_state = 22)
X_tsne = tsne.fit_transform(X)
rfr_tSNE = rfr_algorithm(X_train, X_test, y_train, y_test)

"""Do the best combination of hyperparameters of perplexity, learning rate and number of iterations is [5, 1000, 500]. But it doesn't improve the model, so it makes no sense to apply t-SNE to data prior tp applying the model to data.

**STEP 11**: Cross validation for hyperparameter optimization:

As it was done for the t-SNE, hyperparameter tunning is one option that must be considered for model improvement. Random Forrest model has several hyperparameters that can be tuned for different model purposes. Here below is a plane explanation about all the helpful hyperparameters:

- n_estimators: sets the total number of trees in the forest
- criterion: it is the function for measuring the quality of the split
- max_depth: maximum depth of the tree
- min_samples_split: minimum number of nodes required to split an internal node
- min_samples_leaf: minimum number of samples required to be a leaf node.
- bootstrap: if bootstrap samples are used when building trees

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

https://www.bing.com/videos/riverview/relatedvideo?&q=random+forest+regressor&&mid=E0EA34157561AC0D648AE0EA34157561AC0D648A&&FORM=VRDGAR
"""

param_grid = {
    "n_estimators": [100,300],
    "max_depth": [10,20],
    "min_samples_split": [5,10],
    "min_samples_leaf": [2,4]
}

from sklearn.model_selection import GridSearchCV

model_cv = GridSearchCV(estimator = rf_regressor, param_grid = param_grid, cv = 5, scoring = "neg_mean_squared_error", verbose = 2)

model_cv.fit(X_train, y_train)

# Performing the general error functions and performance metrics:
mse = mean_squared_error(y_test, y_pred) # mean squared error
r2 = r2_score(y_test, y_pred) # R square metric
mae = mean_absolute_error(y_pred, y_test) # Mean absolute error

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared Score: {r2:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")
print("Best Parameters:",model_cv.best_params_)

rfr_crossval = [mse,r2,mae]

"""Ok so neither PCA nor Cross-validation improve significantly my model. I will inspect the age as a function of the sex of the abalones. I will do this because while inspecting the comparisons between the actuals and the predictions I realised that the model had more problems for predicting high age values rather than the normal or short age records.

https://seaborn.pydata.org/generated/seaborn.boxplot.html
"""

# Computing the boxplots for the age observations as a function of the sex of the abalones:
import seaborn as sns

sns.boxplot(data = df, x = "Sex", y = "Age")
plt.title("Ages distribution as a function of the Sex of the abalones")
plt.xlabel("Sex")
plt.ylabel("Age")
plt.axhline(y=10.5, color = "red" )
plt.show()

"""The red line shows a clear spread which suggests the idea of explaining the lack of accuracy while predicting the high age values. So the sex of the abalone for the adults (either female or male) is not representative for describing the age of the abalones. So the age prediction concerningthe young abalones will be predicted better than for the older abalones.

I will try now to apply again a Random Forrest Regressor but through tensor flow:

https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel

https://www.bing.com/videos/riverview/relatedvideo?q=regression+with+an+abalone+dataset&mid=77EA29A29EABC8DEEC7B77EA29A29EABC8DEEC7B&FORM=VIRE
"""

# installing for the tensor flow model Random Forrest Regressor
# !pip install tensorflow_decision_forests

# import tensorflow_decision_forests as tfdf

# # Splitting dataset into training and testing:
# train_ds_pd, test_ds_pd = train_test_split(scaled_df_out.drop("Number of rings", axis = 1), test_size = 0.2, random_state = 42)

# train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label = "Age", task = tfdf.keras.Task.REGRESSION)
# test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label = "Age", task = tfdf.keras.Task.REGRESSION)

# # model configuration:
# model = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)

# # training the model:
# model.fit(train_ds)

# # mmodel compilation and evaluation through the mean square error metric:
# model.compile(metrics =["mse"])
# evaluation = model.evaluate(test_ds, return_dict = True)

# # Storing model predictions and computing the r-squared for the model:
# model_predictions = model.predict(test_ds).flatten()
# y_true = test_ds_pd["Age"].values
# r2 = r2_score(y_true, model_predictions) # R square metric
# mae = mean_absolute_error(y_pred, y_test) # Mean absolute error


# print(f"R-squared Score: {r2:.2f}")
# print(f"Mean Absolute Error: {mae:.2f}")
# print(evaluation)

"""So the model from tensor flow significantly improves the predictions and decreases the MSE. The last thing will be also try to optimize this model parameters. This will be done through the tuner. Keras tuner is a meta-learning algorithm that finds the optimal parameter values of a base learner.  

https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/tuner

First we apply the model with the best performance to the dataset with outliers:
"""

# import tensorflow_decision_forests as tfdf

# # Splitting dataset into training and testing:
# train_ds_pd, test_ds_pd = train_test_split(scaled_df.drop("Number of rings", axis = 1), test_size = 0.2, random_state = 42)

# train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label = "Age", task = tfdf.keras.Task.REGRESSION)
# test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label = "Age", task = tfdf.keras.Task.REGRESSION)

# # Configure the tuner:
# tuner = tfdf.tuner.RandomSearch(num_trials = 20)

# # All the hyperparameters:
# tuner.choice("num_candidate_attributes_ratio", [1.0, 0.8, 0.6]) # this will control the proportion of atttributes during the split
# tuner.choice("use_hessian_gain", [True, False]) # wether to use Hessian Gain of not (2nd derivative of the loss function)
# tuner.choice("split_axis", ["SPARSE_OBLIQUE", "AXIS_ALIGNED"]) # axis splitting specification
# tuner.choice("apply_link_function", [True, False]) # applying a link function to the output or not

# local_search_space = tuner.choice("growing_strategy", ["LOCAL"])
# local_search_space.choice("max_depth", [4,5,6,7,8,9,10]) #limits the tree depth to the LOCAL

# best_first_global_search_space = tuner.choice("growing_strategy", ["BEST_FIRST_GLOBAL"], merge=True)
# best_first_global_search_space.choice("max_num_nodes", [16, 32, 64, 128, 256]) # limits the maximum number of nodes in the forest

# tuner.choice("shrinkage", [0.01, 0.05, 0.1, 0.2, 0.5]) # learning rate
# tuner.choice("subsample", [0.5, 0.7, 1.0]) #samples fraction for bootstrap rounds
# tuner.choice("use_regularization", [True, False]) # use of regularization

# tuner.choice("num_trees", [10, 50, 100, 200, 300]) # number of trees in the forest
# tuner.choice("bootstrap_training", [True, False]) # bootstrap in the training or not

# tuner.choice("l2_regularization", [0.0, 0.1, 0.01, 0.001])#regularization options

# # model configuration:
# model = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)

# # training the model:
# model.fit(train_ds)

# # mmodel compilation and evaluation through the mean square error metric:
# model.compile(metrics =["mse", "mae"])
# evaluation = model.evaluate(test_ds, return_dict = True)

# # Storing model predictions and computing the r-squared for the model:
# model_predictions = model.predict(test_ds).flatten()
# y_true = test_ds_pd["Age"].values
# r2 = r2_score(y_true, model_predictions) # R square metric

# print(f"R-squared Score: {r2:.2f}")
# print(evaluation["mse"], evaluation["mae"])

# rfr_keras_scaled = [evaluation["mse"], r2, evaluation["mae"]]

"""**STEP 12**: Models performance towards the dataset with and without the outliers

We then apply the keras model to the model without outliers:
"""

# import tensorflow_decision_forests as tfdf

# # Splitting dataset into training and testing:
# train_ds_pd, test_ds_pd = train_test_split(scaled_df_out.drop("Number of rings", axis = 1), test_size = 0.2, random_state = 42)

# train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label = "Age", task = tfdf.keras.Task.REGRESSION)
# test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label = "Age", task = tfdf.keras.Task.REGRESSION)

# # Configure the tuner:
# tuner = tfdf.tuner.RandomSearch(num_trials = 20)

# # All the hyperparameters:
# tuner.choice("num_candidate_attributes_ratio", [1.0, 0.8, 0.6]) # this will control the proportion of atttributes during the split
# tuner.choice("use_hessian_gain", [True, False]) # wether to use Hessian Gain of not (2nd derivative of the loss function)
# tuner.choice("split_axis", ["SPARSE_OBLIQUE", "AXIS_ALIGNED"]) # axis splitting specification
# tuner.choice("apply_link_function", [True, False]) # applying a link function to the output or not

# local_search_space = tuner.choice("growing_strategy", ["LOCAL"])
# local_search_space.choice("max_depth", [4,5,6,7,8,9,10]) #limits the tree depth to the LOCAL

# best_first_global_search_space = tuner.choice("growing_strategy", ["BEST_FIRST_GLOBAL"], merge=True)
# best_first_global_search_space.choice("max_num_nodes", [16, 32, 64, 128, 256]) # limits the maximum number of nodes in the forest

# tuner.choice("shrinkage", [0.01, 0.05, 0.1, 0.2, 0.5]) # learning rate
# tuner.choice("subsample", [0.5, 0.7, 1.0]) #samples fraction for bootstrap rounds
# tuner.choice("use_regularization", [True, False]) # use of regularization

# tuner.choice("num_trees", [10, 50, 100, 200, 300]) # number of trees in the forest
# tuner.choice("bootstrap_training", [True, False]) # bootstrap in the training or not

# tuner.choice("l2_regularization", [0.0, 0.1, 0.01, 0.001])#regularization options

# # model configuration:
# model = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)

# # training the model:
# model.fit(train_ds)

# # mmodel compilation and evaluation through the mean square error metric:
# model.compile(metrics =["mse", "mae"])
# evaluation = model.evaluate(test_ds, return_dict = True)

# # Storing model predictions and computing the r-squared for the model:
# model_predictions = model.predict(test_ds).flatten()
# y_true = test_ds_pd["Age"].values
# r2 = r2_score(y_true, model_predictions) # R square metric

# print(f"R-squared Score: {r2:.2f}")
# print(evaluation["mse"], evaluation["mae"])

# rfr_keras_outliers = [evaluation["mse"], r2, evaluation["mae"]]

"""Now let's apply the models with sklearn:

First with the scaled dataframe with the outliers:


"""

from sklearn.model_selection import train_test_split

# Removing the Age and the number of rings as the predictors -> include the number of rings will be redundant in the analysis because we obtained the age through them.
X = scaled_df.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df["Age"] # predicting the age
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 19) # train and test spliting

# Importing the needed libraries:
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# These are the optimal hyperparameters for the Random Forrest model
# {'max_depth': 10,
#  'min_samples_leaf': 4,
#  'min_samples_split': 10,
#  'n_estimators': 300}


# Calling the model with the best performing parameters:
rf_regressor = RandomForestRegressor(n_estimators = 300, max_depth = 10, min_samples_leaf = 4, min_samples_split = 10, random_state = 42)
rf_regressor.fit(X_train, y_train) # training the model
y_pred = rf_regressor.predict(X_test) # making predictions on the model

# Performing the general error functions and performance metrics:
mse = mean_squared_error(y_test, y_pred) # mean squared error
r2 = r2_score(y_test, y_pred) # R square metric
mae = mean_absolute_error(y_pred, y_test) # Mean absolute error

# Printing the metrics:
print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared Score: {r2:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")

# Printing some comparison between the actual age observations and the predicted
comparison = pd.DataFrame({"Actual": y_test.reset_index(drop = True), "Predicted": y_pred})

print(comparison)

rfr_sklearn_scaled = [mse, r2, mae]

"""Then the model is computed with the scaled dataframe without outliers:"""

from sklearn.model_selection import train_test_split

# Removing the Age and the number of rings as the predictors -> include the number of rings will be redundant in the analysis because we obtained the age through them.
X = scaled_df_out.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df_out["Age"] # predicting the age
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 19) # train and test spliting

# Importing the needed libraries:
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# These are the optimal hyperparameters for the Random Forrest model
# {'max_depth': 10,
#  'min_samples_leaf': 4,
#  'min_samples_split': 10,
#  'n_estimators': 300}


# Calling the model with the best performing parameters:
rf_regressor = RandomForestRegressor(n_estimators = 300, max_depth = 10, min_samples_leaf = 4, min_samples_split = 10, random_state = 42)
rf_regressor.fit(X_train, y_train) # training the model
y_pred = rf_regressor.predict(X_test) # making predictions on the model

# Performing the general error functions and performance metrics:
mse = mean_squared_error(y_test, y_pred) # mean squared error
r2 = r2_score(y_test, y_pred) # R square metric
mae = mean_absolute_error(y_pred, y_test) # Mean absolute error

# Printing the metrics:
print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared Score: {r2:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")

# Printing some comparison between the actual age observations and the predicted
comparison = pd.DataFrame({"Actual": y_test.reset_index(drop = True), "Predicted": y_pred})

print(comparison)

rfr_sklearn_outliers = [mse, r2, mae]

"""# Part 2:

 In this part, you are required to compare the performance of two prediction algorithms on the same
 task (i.e. predicting the age of Abalone). The first of these algorithms should be drawn from the
 lecture material, the second should be from your own reading (i.e. not in the teaching materials for
 this module). Your comparison should consist of both a critical comparison (based on your knowledge
 of these techniques), as well as an empirical (quantitative) comparison of their performance. All of the
 empirical comparisons you make should be included in the Python file you submit.

 Note that you are not required to use the same model in Part 2 that you used in Part 1, but it would be
 sensible for you to do so in order to save time and e!ort.
 You will attract marks based on your independent research and selection of an appropriate technique
 from the literature, your justification and critical comparison of your selected approaches, and your
 approach when empirically comparing their performance.

In that part the model which will be implemented is the L1 Multi-Kernel Learning Support Vector Regression Ensemble Algorithm. This model is applied for either linear and non-linear correlation among features and also in high dimensional datasets. The model objectives are to optimize the parameters so that are involved in the design of support vector regression. Non-linear regression is achieved by using kernel functions that map the sample to the feature space.

L1 Multi-Kernel Learning Support Vector Regression Ensemble Algorithm tries to apply to the features a combination of different kernel functions from the basic kernel functions set. The action of combining more than one kernel functions is thought to have a better effect on the model prediction performance.

The model will use the same pre-processing used for the Random Forrest Regressor used in Part 1.

https://arxiv.org/pdf/2105.10373

https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9714461


**STEP 1**: Perform the L1 Multi-Kernel Learning Support Vector Regression Ensemble Algorithm

In the cell below the goal is to create different combinations of kernels which will be implemented in the Regression Voting function that will combine the results of all the different created models to boost the performance of the ensemble model.

https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html

https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py

https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html
"""

# Empty lists to store all the kerne function possible:
rbf_kernels = []
polynomial_kernels = []
linear_kernels = []

# Different parameters to create models:
C = [0.1,1,10] # it is the regularization parameter
epsilon = [0.01,0.1,0.2] # epsilon-tube within which no penalty
# is associated in the training loss function with points predicted within a distance epsilon from the actual value
gamma = ["scale", "auto", 0.1, 0.5] # it is the kernel coefficient
degree = [2,3,4,5] # different degree values for the polynomial kernel

# Creating all the possible combinations of parameters the 3 type of kernels:
for c in C:
  for e in epsilon:
    for g in gamma:
      rbf_kernels.append([c,e,g])
      linear_kernels.append([c,e,g])
    for d in degree:
      polynomial_kernels.append([c,e,d])

"""The SVR model is computed with the scaled dataframe:"""

from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge

# Removing the Age and the number of rings as the predictors -> include the number of rings will be redundant in the analysis because we obtained the age through them.
X = scaled_df.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df["Age"] # predicting the age

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import random as rd
r_max = 0

# Looking for the best ensemble model, iterating a given amount of times:
num_iters = 50
for i in range(num_iters):
  rbf = rd.choice(rbf_kernels)
  linear = rd.choice(linear_kernels)
  poly = rd.choice(polynomial_kernels)

  svr_rbf = SVR(kernel = "rbf", C = rbf[0], gamma = rbf[2], epsilon = rbf[1])
  svr_linear = SVR(kernel = "linear", C = linear[0], gamma = linear[2], epsilon = linear[1])
  svr_poly = SVR(kernel = "poly", C = poly[0], degree = poly[2], epsilon = poly[1])

  ridge_l1 = Ridge(alpha = 0.1) # Ridge regression L1 regularized

  # Ensemble model:
  ensemble_model = VotingRegressor(estimators=[('svr_rbf', svr_rbf),
                                             ('svr_poly', svr_poly),
                                             ('svr_linear', svr_linear),
                                             ('ridge_l1', ridge_l1)])

  ensemble_model.fit(X_train, y_train) # ensemble model training
  y_pred = ensemble_model.predict(X_test) #ensemble model predictions
  r2 = r2_score(y_test, y_pred) # R square metric

  if r2 > r_max: # storing the r_square value in case it is the best
    r_max = r2
    best_kernels = [rbf,linear,poly] # storing the kernels giving the best model performance
    best_mse = mean_squared_error(y_test, y_pred) # taking the minimum mean squared error at the moment
    mae = mean_absolute_error(y_pred, y_test) # Mean absolute error
# Showing the best results for the ensemble models:
print(f"Mean Squared Error of the Ensemble Model: {best_mse}")
print(f"Mean Squared Error of the Ensemble Model: {mae}")
print(f"R-squared Score: {r_max:.2f}")
print("Best kernels parameters: ", best_kernels)

svr_sklearn_scaled = [best_mse, r2, mae]

"""The SVR model is computed with the scaled dataframe without outliers:"""

from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge

# Removing the Age and the number of rings as the predictors -> include the number of rings will be redundant in the analysis because we obtained the age through them.
X = scaled_df_out.drop(["Number of rings", "Age"], axis = 1)
y = scaled_df_out["Age"] # predicting the age

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# Applying the scaler to the training and test sets:
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import random as rd
r_max = 0

# Looking for the best ensemble model, iterating a given amount of times:
num_iters = 50
for i in range(num_iters):
  rbf = rd.choice(rbf_kernels)
  linear = rd.choice(linear_kernels)
  poly = rd.choice(polynomial_kernels)

  svr_rbf = SVR(kernel = "rbf", C = rbf[0], gamma = rbf[2], epsilon = rbf[1])
  svr_linear = SVR(kernel = "linear", C = linear[0], gamma = linear[2], epsilon = linear[1])
  svr_poly = SVR(kernel = "poly", C = poly[0], degree = poly[2], epsilon = poly[1])

  ridge_l1 = Ridge(alpha = 0.1) # Ridge regression L1 regularized

  # Ensemble model:
  ensemble_model = VotingRegressor(estimators=[('svr_rbf', svr_rbf),
                                             ('svr_poly', svr_poly),
                                             ('svr_linear', svr_linear),
                                             ('ridge_l1', ridge_l1)])

  ensemble_model.fit(X_train, y_train) # ensemble model training
  y_pred = ensemble_model.predict(X_test) #ensemble model predictions
  r2 = r2_score(y_test, y_pred) # R square metric

  if r2 > r_max: # storing the r_square value in case it is the best
    r_max = r2
    best_kernels = [rbf,linear,poly] # storing the kernels giving the best model performance
    best_mse = mean_squared_error(y_test, y_pred) # taking the minimum mean squared error at the moment
    mae = mean_absolute_error(y_pred, y_test) # Mean absolute error
# Showing the best results for the ensemble models:
print(f"Mean Squared Error of the Ensemble Model: {best_mse}")
print(f"Mean Squared Error of the Ensemble Model: {mae}")
print(f"R-squared Score: {r_max:.2f}")
print("Best kernels parameters: ", best_kernels)

svr_sklearn_outliers = [best_mse, r2, mae]

"""The code below tries to reproduce the algorithm described in https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9714461 as the Ada-SVR-R:"""

# from sklearn.svm import SVR
# # The input of the model should be the training samples, the parameters of SVR and the desired error threshold:

# def svr_r(X_train, y_train, X_test, y_test, n_iter, threshold, kernel):
#   n_samples = X_train.shape[0] #taking as the training set number of samples the number of samples
#   weights = np.ones(shape = n_samples)/n_samples # initializing the weights
#   ensemble = []

#   for t in range(n_iter):
#     lamda = weights/np.sum(weights) # weights normalization
#     svr = SVR(kernel = kernel) # building the SVR model
#     svr.fit(X_train, y_train, sample_weight = lamda) # training the data with the model

#     train_predictions = svr.predict(X_train) # predictions checked on the training set
#     error = np.sum(lamda*(np.abs(y_train - train_predictions) > threshold)) # calculation of the weighted classification-type loss

#     if error > 1: # algorithm stoping condition imposed
#       print(error)
#       break

#     alpha_t = 0.5*np.log((1-error)/error) # base learner calculation

#     # weights update of the training samples:

#     if np.abs(y_train - train_predictions) < threshold:
#       weights = weights * np.exp(-alpha_t)

#     else:
#       weights = weights * np.exp(alpha_t)
#     print(svr)
#     ensemble.append(svr) #appending the model

#   def final_ensemble(X):
#     predictions = np.array([model.predict(X) for model in ensemble])
#     return np.mean(predictions, axis=0)

#   # Evaluate on test data
#   y_test_pred = final_ensemble(X_test)
#   print(y_test_pred)
#   # mse = mean_squared_error(y_test, y_test_pred)
#   # print(f"Mean Squared Error on Test Set: {mse}")

#   return final_ensemble


# from sklearn.model_selection import train_test_split

# # Removing the Age and the number of rings as the predictors -> include the number of rings will be redundant in the analysis because we obtained the age through them.
# X = scaled_df_out.drop(["Number of rings", "Age"], axis = 1)
# y = scaled_df_out["Age"] # predicting the age

# scaler = StandardScaler() # this stadardizes the features by removing the mean and scalling to unit variance
# # Applying the scaler to the training and test sets:
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 19) # train and test spliting

# svr_r(X_train, y_train, X_test, y_test, 50, 0.1, "rbf")

"""## Comparison between all the models and cases:"""

# Data for models
models = ['svr_sklearn_scaled', 'svr_sklearn_outliers', 'rfr_sklearn_scaled', 'rfr_sklearn_outliers',
          'rfr_keras_scaled', 'rfr_keras_outliers']

# Corresponding metrics: [Mean Squared Error, R Squared, Mean Absolute Error]
metrics = [
    [5.187495827322343,  0.57, 1.5509241509269083],
    [3.112501182594816, 0.48, 1.3274984510544987],
    [5.335793147751175, 0.5329204756706873, 1.629125942386252],
    [2.772795044861933, 0.5015284090268541, 1.3094325824495565],
    [4.875019550323486, 0.5993324932728782, 1.5277905464172363],
    [2.8077337741851807, 0.531329274775153, 1.3089615106582642]
]

# Create a DataFrame
df = pd.DataFrame(metrics, columns=["Mean Squared Error", "R Squared", "Mean Absolute Error"], index=models)

# Round to 2 decimal places
df = df.round(2)

# Display the table
print(df)

"""The model performances are compared in the report."""